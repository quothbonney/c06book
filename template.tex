\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amssymb, amsthm, amsmath}
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
  
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

\makeatother

\usepackage{wrapfig}
\usepackage{quick}


\linespread{1.03} % give a little extra room
\setlength{\parindent}{0.2in} % reduce paragraph indent a bit
\setcounter{secnumdepth}{2} % no numbered subsubsections
\setcounter{tocdepth}{2} % no subsubsections in ToC
\lhead{\sffamily The 18.C06 Master Document}
\begin{document}

% make title page
\thispagestyle{empty}
\bigskip 
\vspace{0.1cm}

\begin{center}
		{\fontsize{20}{20} \selectfont \bf \sffamily The 18.C06 Master Document}
	\vskip 12pt
		{\fontsize{18}{18} \selectfont \rmfamily Jack D.V. Carson}
	\vskip 6pt
		{\fontsize{14}{14} \selectfont \ttfamily jdcarson@mit.edu}
	\vskip 24pt
\end{center}


% make table of contents
\microtoc
\section{Linear Algebra}
This document assumes you already have a reasonable familiarity with the most basic ideas of linear algebra (i.e. What is a matrix? What is matrix multiplication? Why would one be compelled to multiply a matrix?). From hence, we shall begin with the beginning.
\subsection{The Basics}
\begin{itemize}
	\item    In linear algebra, we most often want to express some situation in terms of a formula $Ax=b$ for a matrix $A$, and vectors $x, b$. Then, once we have expressed them in this form, we wish to solve for the values of $x$ that give $b$. The simplest way to do this is with \textbf{Gaussian Elimination} wherein rows are added to one another or multiplied by scalar constants to give a \textbf{diagonal matrix}. This algorithm always goes columnwise. I.e. start from the first column and try and clear everything such that everything below the pivot is 0. This may not always be the fastest approach but it is consistent. From this diagonal matrix, it is simple to \textbf{backsubstitute} to get the complete values of $x$.
		$$\begin{bmatrix}[ccc|c]
  1 & 3 & 1 & 9\\
1 & 1 & -1 & 1\\
3 & 11 & 6 & 35
\end{bmatrix} \xrightarrow{\substack{r_{2}-r_{1} \\ r_{3}-3r_{1}}} \begin{bmatrix}[ccc|c]
  1 & 3 & 1 & 9\\
0 & -2 & -2 & -8\\
0 & 2 & 3 & 8
\end{bmatrix} \xrightarrow{r_{3}+r_{2}}\begin{bmatrix}[ccc|c]
  1 & 3 & 1 & 9\\
0 & -2 & -2 & -8\\
0 & 0 & 1 & 0
\end{bmatrix}$$
Here we have gone from the augmented matrix $[ A | b]$ to a much nicer $[U | b]$. By recalling that matricies are just an abstraction of linear functions, we can look at the last row and say that $x_{3}=0$. We can \textit{propogate} this upwards to say therefore that $-2x_{2}+0=-8 \to x_{2}=4$
\item If we get a 0 in a pivot position, we say that a matrix is \textbf{singular}. In this case $Ax=b$ may not have a solution, or it may have infinite solutions. But it certainly does not have a single solution. All linear equations of this type either have 0, 1, or $\infty$ solutions.
\item It wouldn't hurt us to brush up on matrix multiplication either. It turns out there are many ways to think about this operation.
	\begin{enumerate}
		\item \textbf{Entry-wise:} For each $1 \leq i \leq m$ and $1 \leq j \ldq p$ we have
			$$C_{ij}= \sum_{k=1}^{n} A_{ik}B_{kj}$$
		\item \textbf{Inner product:} $C_{ij}$ is the dot product (also known as ``inner product'') of the $i$th row in $A$ and the $j$th column in $B$. For instance,
			$$C = \begin{bmatrix}
			  \horzbar & x_{1} & \horzbar\\
			  \horzbar & x_{1} & \horzbar\\
					   & \vdots & \\
			  \horzbar & x_{1} & \horzbar
			\end{bmatrix} \begin{bmatrix}
			  \vertbar & \vertbar & &\vertbar \\
			  b_{1} & b_{2} & \cdots & b_{p} \\
			  \vertbar & \vertbar & &\vertbar 
			\end{bmatrix} = \begin{bmatrix}
			  x_{1} \cdot b_{1} & x_{1} \cdot b_{2} & \cdots  & x_{1} \cdot b_{p}\\
			x_{2} \cdot b_{1} & x_{2}\cdot b_{2} & \cdots & b_{2} \cdot b_{p}\\
			\vdots & \vdots & \ddots & \vdots\\
			x_{m}\cdot b_{1} & x_{m} \cdot b_{2} & \cdots & x_{m} \cdot b_{p}
			\end{bmatrix}$$
		\item \textbf{Column-wise: }the $j$th column of a matrix $C$ is the matrix-vector product of $A$ and the $j$th column of $B$. For instance,
			$$C = A \begin{bmatrix}
			  \vertbar & \vertbar &  & \vertbar\\
			b_{1} & b_{2} & \cdots & b_{p}\\
			  \vertbar & \vertbar &  & \vertbar
			\end{bmatrix} = \begin{bmatrix}   
			  \vertbar & \vertbar &  & \vertbar\\
			Ab_{1} & Ab_{2} & \cdots & Ab_{p}\\
			  \vertbar & \vertbar &  & \vertbar
			\end{bmatrix}$$
		\item \textbf{Outer product:} $C$ is the sum of the product of $i$th column of $A$ and the $i$th row of $B$ such as
			$$C=\begin{bmatrix}
			  \vertbar & \vertbar &  & \vertbar\\
			a_{1} & a_{2} & \cdots & a_{n}\\
			\vertbar & \vertbar &  & \vertbar
			\end{bmatrix}
			\begin{bmatrix}
			  \horzbar & y_{1} & \horzbar\\
			\horzbar & y_{2} & \horzbar\\
			 & \vdots & \\
			\horzbar & y_{n} & \horzbar
			\end{bmatrix} = 
			\sum_{i=1}^{n} \begin{bmatrix}
			  \vertbar \\
			a_{i}\\
			\vertbar
			\end{bmatrix} \begin{bmatrix}
			  \horzbar & y_{i} & \horzbar
			\end{bmatrix}$$
	\end{enumerate}
	\begin{ideabox}[Properties of Matrix Multiplication]\quad
		\begin{enumerate}
			\item \textbf{Associative: } $A(BC) = (AB)C$
			\item \textbf{Distributive: } $A(B+C) = AB + AC \iff (A+B)C = AC+BC$
			\item \textbf{Non-commutative: } $AB \neq BA$
			\item \textbf{Identity: } $IA = AI = A$
		\end{enumerate} 
	\end{ideabox}
\item Although it can often be lost in the abstraction of mathematics, matricies really, truly are \textbf{linear operators}. They transform spaces and vectors to other spaces and other vectors. As an example consider 2023 Recitation 1 Problem 1
	\begin{examplebox}[1.1]
		\begin{enumerate}
	\item Find a $2 \times 2$ matrix such that when you multiply a 2-D vector by it, the result is a reflection of the vector across the origin
	\item Find a $3 \times 3$ matrix such that when you multiply a 3-D vector by it, it swaps the second and third coordinates.
	\item If you have a $4 \times 4$ matrix $A$, find a 4-D vector $x$ such that $Ax$ is the second column of $A$.
		    
		\end{enumerate}
		\begin{solution}[1.1] \quad\vspace{-0.5cm}
		\begin{enumerate}
		    \item To reflect across the origin, $x \mapsto -x$, $y \mapsto -y$. Therefore, $A = \begin{bmatrix}
	  -1 & 0\\
	0 & -1
	\end{bmatrix}$.
\item Here $\begin{bmatrix}
  x\\
y\\
z
\end{bmatrix} \mapsto \begin{bmatrix}
  x\\
z\\
y
\end{bmatrix} = x \begin{bmatrix}
  1\\
0\\
0
\end{bmatrix} + y\begin{bmatrix}
  0\\
0\\
1
\end{bmatrix} + z \begin{bmatrix}
  0\\
1\\
0
\end{bmatrix} = \underbrace{\begin{bmatrix}
  1 & 0 & 0\\
0 & 0 & 1\\
0 & 1 & 0
\end{bmatrix}}_A \begin{bmatrix}
  x\\
y\\
z
\end{bmatrix}$

Here $A$ is a \textbf{permutation matrix}, as it permutes one or more of the variables. Permutation matricies have a variety of desirable properties such as $A^{-1}= A^{T}$
\item Since we are only interested in the second column, we want the products $\mathbf{c_{i}}x_{i}=0$ for $i=1,3,4$ to be 0. The only way to guaruntee this is to specify $x_{1,3,4}=0$ and $x_{2}=1$. Therefore $\mathbf{x}=\begin{bmatrix}
  0 & 1 & 0 & 0
\end{bmatrix}^{T}$
		\end{enumerate}
	\end{solution}
	\begin{takeaways}[1.1]\quad\vspace{-0.5cm}
	    \begin{enumerate}
			\item All of the matrix formulations here are useful to recognize, particularly the intuition of part (3). This will be seen many times.
			\item One useful way to interpret the result in part (2) is the fact that left-multiplied matricies operate on the rows of that which they multiply. Right-multiplied matricies operate on columns. Because $\mathbf{x}$ has only one column, right-multiplication is nonsensical. 

	    \end{enumerate}
	\end{takeaways}
	\end{examplebox}
	\begin{examplebox}[1.2]\quad
	    \begin{enumerate}
			\item Find a $3 \times 3$ matrix $P$ such that in $B=PA$ is the result of subtracting the second row from the third row of $A$ and then swapping the first and second rows.
			\item Find a $4 \times 4$ matrix that right multiplies $A$ such that result $C=AQ$ is $A$ after dividing the first column by two, and then adding the first column to the second and third columns
			\item Does the order of performing the operations in (1) and (2) matter?
	    \end{enumerate}
		\begin{solution}[1.2]\quad\vspace{-0.5cm}
		    \begin{enumerate}
				\item $P=\underbrace{\begin{bmatrix}
				  1 &  & \\
				 & 1 & \\
				 & -1 & 1
		  \end{bmatrix}}_{r_{3}=r_{3}-r_{2}}\underbrace{ \begin{bmatrix}
		     & 1 & \\
		  1 &  & \\
		   &  & 1
  \end{bmatrix}}_{\text{swap } \bm{c}_{1}, \bm{c}_{2}} = \begin{bmatrix}
   0  & 1 &0 \\
  1 & 0 &0 \\
  0 & -1 & 1
  \end{bmatrix}$
\item $Q = \begin{bmatrix}
  1/2 &  &  & \\
 & 1 &  & \\
 &  & 1 & \\
 &  &  & 1
\end{bmatrix} \begin{bmatrix}
  1 & 1 & 1 & \\
 & 1 &  & \\
 &  & 1 & \\
 &  &  & 1
\end{bmatrix} = \begin{bmatrix}
  1/2 & 1/2 & 1/2 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{bmatrix}$. This multiplication should be obvious by the column-wise definition above.
\item Although it might look like we are dealing with the commutative propety here, we are really dealing with the associative property. In (1). $B=(P_{1}P_{2})A = P_{1}(P_{2} A)$. So the order of operations does not matter. In (2) it is the same $C=A(Q_{1}Q_{2}) = (AQ_{1})Q_{2}$
		    \end{enumerate}
		\end{solution}
		\begin{takeaways}[1.2]\quad\vspace{-0.4cm}
		    \begin{itemize}
				\item Remember that left multiplication always affects rows only, and that right multiplication affects columns only. Complex operations can be formed by chaining linear operators together.
				\item Matrix multiplication is associative! When doing these chained operations, the order does not matter.
				\item Remember the forms that these kinds of matricies take. They are not always obvious.
		    \end{itemize}
		\end{takeaways}
	\end{examplebox}
\item Before we close up, there are some other noteworthy operations we can perform with matricies that will follow us around. The \textbf{transpose} of a matrix $A^T$ ``flips'' a matrix $A$ such that 
	$$A = \begin{bmatrix}
	  1 & 2 & 3\\
	4 & 5 & 6
	\end{bmatrix}\quad\quad A^{T} = \begin{bmatrix}
	  1 & 4\\
	2 & 5\\
	3 & 6
	\end{bmatrix}$$
	and with it we can define certain identities such as $\boxed{(A^T)_{i,j} = A_{j, i}}$ and $\boxed{(AB)^{T} = B^{T}A^{T}}$.Finally, we can define the dot product between two vectors $\mathbf{a} \cdot \mathbf{b} = \mathbf{a}^{T}\mathbf{b}$. All three of these facts will show up constantly. 
\item Last but not least, the \textbf{inverse} $A^{-1}$ of a matrix is the matrix such that $AA^{-1}=A^{-1}A=I$. More interestingly we can say that in a situation $Ax=b$, $x = A^{-1}b$. This matrix inverse does not always exists, and is exceedingly impractical to calculate for large matricies, an issue we will deal with thoroughly in optimization. A matrix is said to be \textbf{invertible} if and only if it is square and full column rank. That is, every column has a pivot. The inverse, if it exists, is always unique. Like the transpose it is subject to the identity $\boxed{(AB)^{-1} = B^{-1}A^{-1}}$
\item We can define more facts that will help us with these two operations. For instance $(A^T)^T$ and $(A+B)^{T} = A^{T}+B^{T}$, which follow directly from the definition of the transpose. More useful is $\boxed{(A^{-1})^{T} = (A^{T})^{-1}}$. All of these identities will help us greatly.
\end{itemize}
\subsection{Vector Spaces}
\begin{ideabox}[Definition]
    A \textbf{vector space} $V$ is a set of elements (e.g., vectors in $\R^{n}$, polynomials, diagonal $2 \times 2$ matricies) defined over a ``field'' $F$ of scalars that are closed under
	\begin{enumerate}
		\item \textbf{Vector Addition: } For any vectors $\bm{u}$ and $\bm{v}$ in $V$, $\bm{u} \pm \bm{v}$ also belongs the vector space $V$.
		\item \textbf{Scalar Multiplication: }For any vector $\bm{v}$ in $V$ and scalar $c$ in $F$, $c\bm{v}$ is an element of the vector space
	\end{enumerate}
\end{ideabox}
\begin{itemize}
	\item If $W \subseteq V$ is also a vector space with respect to the operations in $V$, $W$ is a \textbf{subspace} of $V$. Specifically for any $v, w \in W$, $v+w \in W$ and $cv \in W$. For instance of we define a vector space $\R^2$. $W$ could be $\R^2$ or $W= \{0\}$. A more enlightening sample vector space is a line that passes through the origin (note that a line that does not pass through the origin would not be a vector space since $\alpha v \not \in V$ for $\alpha =0$). Every point on this line is closed under addition and scalar multiplication.
	\item There are an infinite number of obscure facts to say about spaces and subspaces. More interestingly, is how to show something is a space or a substapce. We can be convinced that for subspaces $S_{1}$ and $S_{2}$ $S = S_{1} \cap S_{2}$ is also a subspace by showing that $S$ is closed under addition and scalar multiplication.
		\\\\
		\begin{proof} Assuming that this is true, any $v, w \in S$ will also be $v, w \in S_{1}$ and $v,w \in S_{2}$ by the definition of an intersection. Then $v+w$ will be in	$S_{1}$ and $S_{2}$ by the deifnition of subspace. Thus $v+w \in S_{1} \cap S_{2} =S$ which shows that it is closed under addition. Then we can define $v \in S_{1} \cap S_{2}$ and $\alpha \in \R$. Since $S_{1}$ is a subspace, $\alpha v \in S_{1}$ and $S_{2}$ such that $\alpha v \in S_{1} \cap S_{2}=S$, showing that it is closed under scalar multiplication. 
		\end{proof}
\subsubsection*{Column Space}
\item The \textbf{column space} of an $m \times n$ matrix $A$, denoted $C(A)$ is the set of linear combinations of columns of $A$, also known as the \textbf{span} of $A$. We can define useful facts such as
	\begin{enumerate}
		\item Formally, $C(A) = \{Ax \mid x \in \R^{n}\}$ 
		\item $Ax=b$ has a solution if and only if $b \in C(A)$
		\item If $m=n$, then $A$ is invertible if and only if $C(A) = \R^{n}$
	\end{enumerate}$
\end{itemize}



\end{document}

