\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amssymb, amsthm, amsmath}
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
  
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}
\newcommand*{\wrapitem}{\apptocmd{\labelenumi}{\hskip\leftmargin}{}{}\item\apptocmd{\labelenumi}{\hskip-\leftmargin}{}{}}

\makeatother

\usepackage{wrapfig}
\usepackage{quick}
\usepackage{graphicx}


\linespread{1.03} % give a little extra room
\setlength{\parindent}{0.2in} % reduce paragraph indent a bit
\setcounter{secnumdepth}{2} % no numbered subsubsections
\setcounter{tocdepth}{2} % no subsubsections in ToC
\lhead{\sffamily The 18.C06 Master Document}
\begin{document}

% make title page
\thispagestyle{empty}
\bigskip 
\vspace{0.1cm}

\begin{center}
		{\fontsize{20}{20} \selectfont \bf \sffamily The 18.C06 Master Document}
	\vskip 12pt
		{\fontsize{18}{18} \selectfont \rmfamily Jack D.V. Carson}
	\vskip 6pt
		{\fontsize{14}{14} \selectfont \ttfamily jdcarson@mit.edu}
	\vskip 24pt
\end{center}


% make table of contents
\microtoc
\section{Linear Algebra}
This document assumes you already have a reasonable familiarity with the most basic ideas of linear algebra (i.e. What is a matrix? What is matrix multiplication? Why would one be compelled to multiply a matrix?). From hence, we shall begin with the beginning.
\subsection{The Basics}
\begin{itemize}
	\item    In linear algebra, we most often want to express some situation in terms of a formula $Ax=b$ for a matrix $A$, and vectors $x, b$. Then, once we have expressed them in this form, we wish to solve for the values of $x$ that give $b$. The simplest way to do this is with \textbf{Gaussian Elimination} wherein rows are added to one another or multiplied by scalar constants to give a \textbf{diagonal matrix}. This algorithm always goes columnwise. I.e. start from the first column and try and clear everything such that everything below the pivot is 0. This may not always be the fastest approach but it is consistent. From this diagonal matrix, it is simple to \textbf{backsubstitute} to get the complete values of $x$.
		$$\begin{bmatrix}[ccc|c]
  1 & 3 & 1 & 9\\
1 & 1 & -1 & 1\\
3 & 11 & 6 & 35
\end{bmatrix} \xrightarrow{\substack{r_{2}-r_{1} \\ r_{3}-3r_{1}}} \begin{bmatrix}[ccc|c]
  1 & 3 & 1 & 9\\
0 & -2 & -2 & -8\\
0 & 2 & 3 & 8
\end{bmatrix} \xrightarrow{r_{3}+r_{2}}\begin{bmatrix}[ccc|c]
  1 & 3 & 1 & 9\\
0 & -2 & -2 & -8\\
0 & 0 & 1 & 0
\end{bmatrix}$$
Here we have gone from the augmented matrix $[ A | b]$ to a much nicer $[U | b]$. By recalling that matricies are just an abstraction of linear functions, we can look at the last row and say that $x_{3}=0$. We can \textit{propogate} this upwards to say therefore that $-2x_{2}+0=-8 \to x_{2}=4$
\item If we get a 0 in a pivot position, we say that a matrix is \textbf{singular}. In this case $Ax=b$ may not have a solution, or it may have infinite solutions. But it certainly does not have a single solution. All linear equations of this type either have 0, 1, or $\infty$ solutions.
\item It wouldn't hurt us to brush up on matrix multiplication either. It turns out there are many ways to think about this operation.
	\begin{enumerate}
		\item \textbf{Entry-wise:} For each $1 \leq i \leq m$ and $1 \leq j \ldq p$ we have
			$$C_{ij}= \sum_{k=1}^{n} A_{ik}B_{kj}$$
		\item \textbf{Inner product:} $C_{ij}$ is the dot product (also known as ``inner product'') of the $i$th row in $A$ and the $j$th column in $B$. For instance,
			$$C = \begin{bmatrix}
			  \horzbar & x_{1} & \horzbar\\
			  \horzbar & x_{1} & \horzbar\\
					   & \vdots & \\
			  \horzbar & x_{1} & \horzbar
			\end{bmatrix} \begin{bmatrix}
			  \vertbar & \vertbar & &\vertbar \\
			  b_{1} & b_{2} & \cdots & b_{p} \\
			  \vertbar & \vertbar & &\vertbar 
			\end{bmatrix} = \begin{bmatrix}
			  x_{1} \cdot b_{1} & x_{1} \cdot b_{2} & \cdots  & x_{1} \cdot b_{p}\\
			x_{2} \cdot b_{1} & x_{2}\cdot b_{2} & \cdots & b_{2} \cdot b_{p}\\
			\vdots & \vdots & \ddots & \vdots\\
			x_{m}\cdot b_{1} & x_{m} \cdot b_{2} & \cdots & x_{m} \cdot b_{p}
			\end{bmatrix}$$
		\item \textbf{Column-wise: }the $j$th column of a matrix $C$ is the matrix-vector product of $A$ and the $j$th column of $B$. For instance,
			$$C = A \begin{bmatrix}
			  \vertbar & \vertbar &  & \vertbar\\
			b_{1} & b_{2} & \cdots & b_{p}\\
			  \vertbar & \vertbar &  & \vertbar
			\end{bmatrix} = \begin{bmatrix}   
			  \vertbar & \vertbar &  & \vertbar\\
			Ab_{1} & Ab_{2} & \cdots & Ab_{p}\\
			  \vertbar & \vertbar &  & \vertbar
			\end{bmatrix}$$
		\item \textbf{Outer product:} $C$ is the sum of the product of $i$th column of $A$ and the $i$th row of $B$ such as
			$$C=\begin{bmatrix}
			  \vertbar & \vertbar &  & \vertbar\\
			a_{1} & a_{2} & \cdots & a_{n}\\
			\vertbar & \vertbar &  & \vertbar
			\end{bmatrix}
			\begin{bmatrix}
			  \horzbar & y_{1} & \horzbar\\
			\horzbar & y_{2} & \horzbar\\
			 & \vdots & \\
			\horzbar & y_{n} & \horzbar
			\end{bmatrix} = 
			\sum_{i=1}^{n} \begin{bmatrix}
			  \vertbar \\
			a_{i}\\
			\vertbar
			\end{bmatrix} \begin{bmatrix}
			  \horzbar & y_{i} & \horzbar
			\end{bmatrix}$$
	\end{enumerate}
	\begin{ideabox}[Properties of Matrix Multiplication]\quad
		\begin{enumerate}
			\item \textbf{Associative: } $A(BC) = (AB)C$
			\item \textbf{Distributive: } $A(B+C) = AB + AC \iff (A+B)C = AC+BC$
			\item \textbf{Non-commutative: } $AB \neq BA$
			\item \textbf{Identity: } $IA = AI = A$
		\end{enumerate} 
	\end{ideabox}
\item Although it can often be lost in the abstraction of mathematics, matricies really, truly are \textbf{linear operators}. They transform spaces and vectors to other spaces and other vectors. As an example consider 2023 Recitation 1 Problem 1
	\begin{examplebox}[1.1]
		\begin{enumerate}
	\item Find a $2 \times 2$ matrix such that when you multiply a 2-D vector by it, the result is a reflection of the vector across the origin
	\item Find a $3 \times 3$ matrix such that when you multiply a 3-D vector by it, it swaps the second and third coordinates.
	\item If you have a $4 \times 4$ matrix $A$, find a 4-D vector $x$ such that $Ax$ is the second column of $A$.
		    
		\end{enumerate}
		\begin{solution}[1.1] \quad\vspace{-0.2cm}
		\begin{enumerate}
		    \item To reflect across the origin, $x \mapsto -x$, $y \mapsto -y$. Therefore, $A = \begin{bmatrix}
	  -1 & 0\\
	0 & -1
	\end{bmatrix}$.

\item Here $\begin{bmatrix}
  x\\
y\\
z
\end{bmatrix} \mapsto \begin{bmatrix}
  x\\
z\\
y
\end{bmatrix} = x \begin{bmatrix}
  1\\
0\\
0
\end{bmatrix} + y\begin{bmatrix}
  0\\
0\\
1
\end{bmatrix} + z \begin{bmatrix}
  0\\
1\\
0
\end{bmatrix} = \underbrace{\begin{bmatrix}
  1 & 0 & 0\\
0 & 0 & 1\\
0 & 1 & 0
\end{bmatrix}}_A \begin{bmatrix}
  x\\
y\\
z
\end{bmatrix}$

Here $A$ is a \textbf{permutation matrix}, as it permutes one or more of the variables. Permutation matricies have a variety of desirable properties such as $A^{-1}= A^{T}$
\item Since we are only interested in the second column, we want the products $\mathbf{c_{i}}x_{i}=0$ for $i=1,3,4$ to be 0. The only way to guaruntee this is to specify $x_{1,3,4}=0$ and $x_{2}=1$. Therefore $\mathbf{x}=\begin{bmatrix}
  0 & 1 & 0 & 0
\end{bmatrix}^{T}$
		\end{enumerate}
	\end{solution}
	\begin{takeaways}[1.1]\quad\vspace{-0.5cm}
	    \begin{enumerate}
			\item All of the matrix formulations here are useful to recognize, particularly the intuition of part (3). This will be seen many times.
			\item One useful way to interpret the result in part (2) is the fact that left-multiplied matricies operate on the rows of that which they multiply. Right-multiplied matricies operate on columns. Because $\mathbf{x}$ has only one column, right-multiplication is nonsensical. 

	    \end{enumerate}
	\end{takeaways}
	\end{examplebox}
	\begin{examplebox}[1.2]\quad
	    \begin{enumerate}
			\item Find a $3 \times 3$ matrix $P$ such that in $B=PA$ is the result of subtracting the second row from the third row of $A$ and then swapping the first and second rows.
			\item Find a $4 \times 4$ matrix that right multiplies $A$ such that result $C=AQ$ is $A$ after dividing the first column by two, and then adding the first column to the second and third columns
			\item Does the order of performing the operations in (1) and (2) matter?
	    \end{enumerate}
		\begin{solution}[1.2]\quad\vspace{-0.5cm}
		    \begin{enumerate}
				\item $P=\underbrace{\begin{bmatrix}
				  1 &  & \\
				 & 1 & \\
				 & -1 & 1
		  \end{bmatrix}}_{r_{3}=r_{3}-r_{2}}\underbrace{ \begin{bmatrix}
		     & 1 & \\
		  1 &  & \\
		   &  & 1
  \end{bmatrix}}_{\text{swap } \bm{c}_{1}, \bm{c}_{2}} = \begin{bmatrix}
   0  & 1 &0 \\
  1 & 0 &0 \\
  0 & -1 & 1
  \end{bmatrix}$
\item $Q = \begin{bmatrix}
  1/2 &  &  & \\
 & 1 &  & \\
 &  & 1 & \\
 &  &  & 1
\end{bmatrix} \begin{bmatrix}
  1 & 1 & 1 & \\
 & 1 &  & \\
 &  & 1 & \\
 &  &  & 1
\end{bmatrix} = \begin{bmatrix}
  1/2 & 1/2 & 1/2 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{bmatrix}$. This multiplication should be obvious by the column-wise definition above.
\item Although it might look like we are dealing with the commutative propety here, we are really dealing with the associative property. In (1). $B=(P_{1}P_{2})A = P_{1}(P_{2} A)$. So the order of operations does not matter. In (2) it is the same $C=A(Q_{1}Q_{2}) = (AQ_{1})Q_{2}$
		    \end{enumerate}
		\end{solution}
		\begin{takeaways}[1.2]\quad\vspace{-0.4cm}
		    \begin{itemize}
				\item Remember that left multiplication always affects rows only, and that right multiplication affects columns only. Complex operations can be formed by chaining linear operators together.
				\item Matrix multiplication is associative! When doing these chained operations, the order does not matter.
				\item Remember the forms that these kinds of matricies take. They are not always obvious.
		    \end{itemize}
		\end{takeaways}
	\end{examplebox}
\item Before we close up, there are some other noteworthy operations we can perform with matricies that will follow us around. The \textbf{transpose} of a matrix $A^T$ ``flips'' a matrix $A$ such that 
	$$A = \begin{bmatrix}
	  1 & 2 & 3\\
	4 & 5 & 6
	\end{bmatrix}\quad\quad A^{T} = \begin{bmatrix}
	  1 & 4\\
	2 & 5\\
	3 & 6
	\end{bmatrix}$$
	and with it we can define certain identities such as $\boxed{(A^T)_{i,j} = A_{j, i}}$ and $\boxed{(AB)^{T} = B^{T}A^{T}}$.Finally, we can define the dot product between two vectors $\mathbf{a} \cdot \mathbf{b} = \mathbf{a}^{T}\mathbf{b}$. All three of these facts will show up constantly. 
\item Last but not least, the \textbf{inverse} $A^{-1}$ of a matrix is the matrix such that $AA^{-1}=A^{-1}A=I$. More interestingly we can say that in a situation $Ax=b$, $x = A^{-1}b$. This matrix inverse does not always exists, and is exceedingly impractical to calculate for large matricies, an issue we will deal with thoroughly in optimization. A matrix is said to be \textbf{invertible} if and only if it is square and full column rank. That is, every column has a pivot. The inverse, if it exists, is always unique. Like the transpose it is subject to the identity $\boxed{(AB)^{-1} = B^{-1}A^{-1}}$
\item We can define more facts that will help us with these two operations. For instance $(A^T)^T$ and $(A+B)^{T} = A^{T}+B^{T}$, which follow directly from the definition of the transpose. More useful is $\boxed{(A^{-1})^{T} = (A^{T})^{-1}}$. All of these identities will help us greatly.
\end{itemize}
\subsection{Vector Spaces}
\begin{ideabox}[Definition]
    A \textbf{vector space} $V$ is a set of elements (e.g., vectors in $\R^{n}$, polynomials, diagonal $2 \times 2$ matricies) defined over a ``field'' $F$ of scalars that are closed under
	\begin{enumerate}
		\item \textbf{Vector Addition: } For any vectors $\bm{u}$ and $\bm{v}$ in $V$, $\bm{u} \pm \bm{v}$ also belongs the vector space $V$.
		\item \textbf{Scalar Multiplication: }For any vector $\bm{v}$ in $V$ and scalar $c$ in $F$, $c\bm{v}$ is an element of the vector space
	\end{enumerate}
\end{ideabox}
\begin{itemize}
	\item If $W \subseteq V$ is also a vector space with respect to the operations in $V$, $W$ is a \textbf{subspace} of $V$. Specifically for any $v, w \in W$, $v+w \in W$ and $cv \in W$. For instance of we define a vector space $\R^2$. $W$ could be $\R^2$ or $W= \{0\}$. A more enlightening sample vector space is a line that passes through the origin (note that a line that does not pass through the origin would not be a vector space since $\alpha v \not \in V$ for $\alpha =0$). Every point on this line is closed under addition and scalar multiplication.
	\item There are an infinite number of obscure facts to say about spaces and subspaces. More interestingly, is how to show something is a space or a substapce. We can be convinced that for subspaces $S_{1}$ and $S_{2}$ $S = S_{1} \cap S_{2}$ is also a subspace by showing that $S$ is closed under addition and scalar multiplication.
		\\\\
		\begin{proof} Assuming that this is true, any $v, w \in S$ will also be $v, w \in S_{1}$ and $v,w \in S_{2}$ by the definition of an intersection. Then $v+w$ will be in	$S_{1}$ and $S_{2}$ by the deifnition of subspace. Thus $v+w \in S_{1} \cap S_{2} =S$ which shows that it is closed under addition. Then we can define $v \in S_{1} \cap S_{2}$ and $\alpha \in \R$. Since $S_{1}$ is a subspace, $\alpha v \in S_{1}$ and $S_{2}$ such that $\alpha v \in S_{1} \cap S_{2}=S$, showing that it is closed under scalar multiplication. 
		\end{proof}
	\end{itemize}
		\begin{ideabox}[Common vector spaces]\quad
			\begin{itemize}
			    
		    
\item The \textbf{column space} of an $m \times n$ matrix $A$, denoted $C(A)$ is the set of linear combinations of columns of $A$, also known as the \textbf{span} of $A$. Although the idea may seem a bit abstract, soon we will be very interested in what columnspace a vector is in. We can define useful facts such as
	\begin{enumerate}
		\item Formally, $C(A) = \{Ax \mid x \in \R^{n}\}$ 
		\item $Ax=b$ has a solution if and only if $b \in C(A)$
		\item If $m=n$, then $A$ is invertible if and only if $C(A) = \R ^{n}$
	\end{enumerate}
\item The \textbf{null space} of $A$, denoted $N(A)$ is the set of vectors $x$ such that $Ax = \mathbf{0}$. We can define similar facts
	\begin{enumerate}
		\item $N(A) = \{ x \in \R^{n} \mid Ax = \bm{0} \}$
		\item If $B$ is square and invetible, $N(A)=N(BA)$
		\item If $A \in \R^{n \times n}$, then $C(A) = \R^n$ is equivalent to $N(A) = \{\bm{0}\}$
	\end{enumerate}
\end{itemize}
		\end{ideabox}
		\begin{itemize}
			\subsubsection*{Computing basis of a null space}
		\item Before we start with computation, it helps to briefly examine our matrix. If we are looking at an invertible matrix, then its null space will clearly be $\{ \bm{0}\}$, since there is no non-trivial $x$ that will give $\bm{0}$. Even if $A$ is not invertible, if every column has a pivot, then there are no variables that can move freely. This can happen easily for an ``overdetermined'' system where a matrix has more rows than columns. However, the most interesting case in which $A$ is ``underdetermined'' such that it has many more columns than rows, and thus many free variables.
		\item Our goal for underdetermined systems will be to transform some matrix $A$
			$$A \leadsto U = \begin{bmatrix}
			  U_r & F \\
			  m-r \text{ rows of }0's & \cdots
			\end{bmatrix}$$
		\item Now how do we implement this scary looking transformation? In reality it is quite simple. First transform $A$ into an upper-triangular matrix $U$ with gaussian elimination.

		\begin{align}
       A = \begin{bmatrix}
         1 & 2 & 3 & 1\\
       1 & 4 & 5 & -3\\
       1 & 6 & 7 & -7
       \end{bmatrix}
	   \leadsto
	   \begin{bmatrix}
		   \boxed{1} & 2 & 3 & 1\\
		 & \boxed{2} & 2 & -4\\
	    &  &  & 
	   \end{bmatrix}
   \end{align} 
   Let pivot columns $U_r =\begin{bmatrix}
     1 & 2\\
    & 2
   \end{bmatrix}$ and free columns $F=\begin{bmatrix}
     3 & 1\\
   2 & -4
   \end{bmatrix}$
   such that
       $U=\begin{bmatrix}
         U_r & F\\
		 \mathbf{0} & \mathbf{0}
       \end{bmatrix}$.
   \item Then we can start with the actual linear algebra. To find $N(A)$ we want $Ax = \mathbf{0} \iff Ux = \mathbf{0}$. In order to compute this we define two vectors $\mathbf{p}$ for the coefficients of $x$ by which the pivot columns of $U$ are multiplied, and a vector $f$ for the values of $x$ that multiply the free columns. With this we can say,
   \begin{gather}
	   U=\begin{pmatrix}
         U_r & F\\
		 \mathbf{0} & \mathbf{0}
       \end{pmatrix}
	   \begin{pmatrix}
		   \mathbf{p}\\
		   \mathbf{f}
	   \end{pmatrix}
		= \begin{pmatrix}
		   U_r \mathbf{p} + F\mathbf{f}\\
		   \mathbf{0}
	   \end{pmatrix}
	   = \mathbf{0}  \\
   U_r \mathbf{p} + F\mathbf{f} = \mathbf{0} \\
   \boxed{U_r \mathbf{p} =- F\mathbf{f}}
   \end{gather}
   $U_r$ is guarunteed to be invertible, so we can say
	   $\mathbf{p} = U_{r}^{-1}(-F \mathbf{f})$
   is guarunteed to be uniquely determined for any choice of $\mathbf{f}$. Then $\begin{pmatrix}
	   \mathbf{p}\\
	   \mathbf{f}
   \end{pmatrix}$ is a basis vector of our nullspace. To show this, let's expand the boxed formula above for our example.
   \begin{align}
       \begin{bmatrix}
         1 & 2\\
        & 2
       \end{bmatrix}
	   \begin{bmatrix}
	     p_{1}\\
	   p_{2}
	   \end{bmatrix}
	   =-\begin{bmatrix}
	     3 & 1\\
	   2 & -4
	   \end{bmatrix}
	   \begin{bmatrix}
	     f_{1}\\
	   f_{2}
	   \end{bmatrix}
   \end{align}
\item For an $n$-dimensional null space, we can just make up whatever $n$ linearly independent $\mathbf{f}$ vectors we might fancy. For simplicity, say $\mathbf{f} = \begin{bmatrix}
  1\\
0
\end{bmatrix}$ such that
   \begin{align}
       \begin{bmatrix}
         1 & 2\\
        & 2
       \end{bmatrix}
	   \begin{bmatrix}
	     p_{1}\\
	   p_{2}
	   \end{bmatrix}
	   =\underbrace{\begin{bmatrix}
	     -3 \\ -2\\
 \end{bmatrix}}_{-F \mathbf{f}} \longrightarrow \mathbf{p}= \vtwoC{-1}{-1} \text{ and }x = \begin{bmatrix}
   -1 & -1 & 1 & 0
\end{bmatrix}^{T}
   \end{align}
   Then we can do this again for $\mathbf{f} = \begin{bmatrix}
     0 & 1
   \end{bmatrix}^{T}$ to complete the basis for $N(A)$.
   
\item In the case that the pivot columns are not adjacent, you \textit{can} interlace the $p$ and $f$ elements systematically.

\subsubsection*{Computing the basis of a columnspace}
\item Let's revisit our matrix $A$ from the previous example.
\begin{align}
    A = \begin{bmatrix}
      1 & 2 & 3 & 1\\
    1 & 4 & 5 & -3\\
    1 & 6 & 7 & -7
    \end{bmatrix}
	\leadsto
	U= \begin{bmatrix}
	  1 & 2 & 3 & 1\\
	 & 2 & 2 & -4\\
	 &  &  & 0
	\end{bmatrix}
\end{align}
In $U$ we can easily identify that $c_{1}$ and $c_{2}$ contain our pivots. Therefore, our basis for $C(A)$ is simply columns $c_{1}$ and $c_{2}$ of $A$ itself. Don't forget, though, that $C(A) \neq C(U)$. Rather,
\begin{align}
    C(A) = \text{span}\Biggl\{\begin{bmatrix}
	  1\\
	1\\
	1
	\end{bmatrix}, \begin{bmatrix}
	  2\\
	4\\
	6	
	\end{bmatrix} \Biggl\}
\end{align}

\subsubsection*{Polynomial fitting}
\item The topic of optimization begins with polynomial fitting. This is a task we will revisit extensively in this course. A simple way to fit a polynomial is with a \textbf{Vandermode Matrix}. A degree-2 polynomial follows a form with linear coefficients $p(x) = c_{0} + c_{1}x + c_{2}x^{2}$, leading to the intuition
	$$
	\underbrace{\begin{bmatrix}
	  1 & x_{1} & x_{1}^{2}\\
	1 & x_{2} & x_{2}^{2}\\
	1 & x_{3} & x_{3}^{2}
\end{bmatrix}}_{A} \begin{bmatrix}
	  c_{0}\\
	c_{1}\\
	c_{2}
	\end{bmatrix} = \begin{bmatrix}
	 y_{0}  \\
	 y_{1} \\
	 y_{2}
	\end{bmatrix}
$$
\item This actually gives us a great way to visualize overdetermined and underdetermined systems.
	 $$\begin{aligned}
\begin{tikzpicture}[scale=0.85] 
    \draw[->] (-2,0) -- (2,0) node[right] {$x$}; 
    \draw[->] (0,-1) -- (0,3) node[above] {$y$}; 
    \fill (-1,1) circle (1.5pt);
    \fill (0,0) circle (1.5pt);
    \fill (1,1) circle (1.5pt);
    \draw[blue, thick] plot[domain=-1.5:1.5] (\x,{\x*\x});
\end{tikzpicture}
& \quad\raisebox{5em}{
\begin{bmatrix} 
1 & x_{1} & x_{1}^{2}\\
1 & x_{2} & x_{2}^{2}\\
1 & x_{3} & x_{3}^{2}
\end{bmatrix}
\begin{bmatrix}
c_{0}\\
c_{1}\\
c_{2}
\end{bmatrix} =
\begin{bmatrix}
y_{0} \\
y_{1} \\
y_{2}
\end{bmatrix} \to \underbrace{A \in \R^{3 \times 3}}_{\text{1-sol}} \to N(A) = \{ \bm{0} \}} \\
	\begin{tikzpicture}[scale=0.75]
    % Axes
    \draw[->] (-2,0) -- (4,0) node[right] {$x$};
    \draw[->] (0,-1) -- (0,4) node[above] {$y$};
    
    % Two fixed points
    \fill (0,1) circle (2pt) node[left] {$P_1$};
    \fill (2,2) circle (2pt) node[right] {$P_2$};
    
    % The unique line
    \draw[blue, thick] (-1,0.5) -- (3,2.5) node[right];
    
    % Multiple parabolas passing through the points
    \draw[red] plot[domain=-1:3] (\x,{1 + 0.25*\x^2});
    \draw[red] plot[domain=-1:3] (\x,{1 + 2*\x - 0.75*\x^2});
    
    % Label
    \node[red] at (3,4)
\end{tikzpicture} & \quad\raisebox{5em}{
	\begin{bmatrix}
	  1 & x_{1} & x_{1}^{2}\\
	1 & x_{2} & x_{2}^{2}
	\end{bmatrix} \begin{bmatrix}
	  c_{0}\\
	c_{1}\\
	c_{2}
	\end{bmatrix}
	= \begin{bmatrix}
	  y_{0}\\
	y_{1}
	\end{bmatrix}
	\to \underbrace{A \in \R^{2 \times 3}}_{\text{underdetermined}} \to N(A) \neq \{ \bm{0} \}
}
\end{aligned}

\item As we can see with the figures, if we have two points, we can define exaclty one line. If we have 3 points, we can define exactly 1 parabola. This is a formalization of the intuitive fact that we can draw an infinte number of parabolas to fit two points.

\begin{examplebox}[1.3]\quad
	\begin{enumerate}
		\item If the zero vector is in $C(A)$, then the columns of $A$ are linearly dependent.
		\item The columns of a matrix are a basis for the column space
		\item Define the row space of $A$ as the span of the row vectors. If $A$ is square, then the row space equals the column space.
		\item The row space of $A$ is equal to the column space of $A^{T}$.
		\item If the row space of $A$ equals the column space, then $A^{T} = A$.
		\item A $4 \times 4$ permutation matrix has $C(P) = \R^{4}$.
		\item For $ v \in N(A) $, if $x$ is a solution to $Ax=b$, so is $x + v$
	\end{enumerate}
	\begin{solution}[1.3]\quad\vspace{-0.4cm}
		\begin{enumerate}
			\item \textbf{False.} $A=I$ is a counterexample. The zero vector is in the column span of every matrix!
			\item \textbf{False.} This is true only if the columns are linearly independent. It is clearly not true for underdetermined sysystems where there are more columns than rows.
			\item \textbf{False.} As a counterexample $A = \mtwtw{1}{1}{0}{0}$ where $C(A) = \vtwoC{1}{0}$ and $R(A) = \vtwoC{1}{1}$
			\item \textbf{True.} This is simply the definition of the row space, and the set of rows of $A$ is identical to the set of columns of $A^{T}$.
			\item \textbf{False:} Consider $A = \mtwtw{1}{2}{3}{4}$. Both $C(A) = \R^{2}$ and $R(A) = \R^{2}$. However, $A \neq A^{T}$.
			\item \textbf{True:} A permutation space simply permutes the order of the variables. If it were to not be in $\R^{4}$, it would somehow lose one of the dimensions.
			\item \textbf{True:} If $v \in N(A)$, $Av = \mathbf{0}$. Therefore $A(x+v) = Ax+av =b$ which is defined in $C(A)$ \end{enumerate}
	    
	\end{solution}
	\begin{takeaways}[1.3]\quad\vspace{-0.3cm}
		\begin{enumerate}
		    \item We will see the row space $C(A^{T})$ in more detail soon. This problem examines simple properties that it has. It is essential to remember that $C(A) \neq C(A^{T})$, even if they have the same dimension.
			\item Always look for simple matricies of size $2 \times 2$ or smaller tos how counterexamples. Most false properties will collapse even by this size.
		\end{enumerate}
	\end{takeaways}
    
\end{examplebox}

\begin{examplebox}[1.4]\quad
$AB = 0$ (the zero matrix) for matricies $A$ and $B$. If the null space of $\_\_\_$ is $\{=, \subseteq, \supseteq \}$ the column space of $\_\_\_$?
\begin{solution}[1.4]\quad\vspace{-0.3cm}
	Recalling our ``Column-Wise`` definition for matrix multiplication
			$$AB = \begin{bmatrix}   
			  \vertbar & \vertbar &  & \vertbar\\
			Ab_{1} & Ab_{2} & \cdots & Ab_{p}\\
			  \vertbar & \vertbar &  & \vertbar
			\end{bmatrix}$$
			$AB=0$ only if every column of $B$ is in the null space of $A$. Therefore the $N(A)$ must contain any possible linear combination of $B$ and possibly more. Therefore $N(B) \supseteq C(A)$
\end{solution} 
\end{examplebox}

\subsection{Factorization (pt. 1)}
\item Before we get to factorization, we should define some simple concepts that have already appeared. For instance, the \textbf{rank} of a matrix is simply the number of pivots. Simply, $\operatorname{rank} A = \dim C(A) = \dim C(A^{T})$, which should be intuitive. We can also define the \textbf{rank-nullity theorem} as $\dim N(A) = n-r$ for $r$ pivots and $n$ columns. Although we do not yet have the tools to introduce them all now, in this document we will examine \textbf{four critical factorizations}
	\begin{align}
		A &= LU \\
		A &= QR \\
		S &= Q \Lambda Q^{T} \\
		A &= U \Sigma V^{T}
	\end{align}
	\subsubsection*{LU Factorization}
\item It turns out that matricies can be described in many ways. Some of these ways can help you solve difficult problems. The first of these is the $\bm{LU}$\textbf{-factorization}, which relates $A$ to its upper-triangular form $U$. In the process, it tells us about the inverse of a matrix. 
\item The crux of the factorization is just Gaussian elimiation of $A \leadsto U$, but storing the coefficients of each row operation in a left-multipliying matrix $L$ 
\begin{align}
	A &= \begin{bmatrix} \color{blue}{1} & 2 & 0 \\ 2 & 5 & 1 \\ -3 & 1 & -1 \end{bmatrix} \stackrel{r_2 - \color{red}{2}r_1}{\stackrel{r_3 + {\color{red}{3}}r_1}{\longrightarrow}}
    \begin{bmatrix} \color{blue}{1} & 2 & 0 \\ 0 & \color{blue}{1} & 1 \\  0 & 7 & -1 \end{bmatrix} \stackrel{r_3 - \color{red}{7}r_2}{\longrightarrow}
    \begin{bmatrix} \color{blue}{1} & 2 & 0 \\ 0 & \color{blue}{1} & 1 \\  0 & 0 & \color{blue}{-8} \end{bmatrix} = U  \\
	\implies L &= \begin{bmatrix} 1 & & \\ \color{red}{+2} & 1 & \\ \color{red}{-3} & \color{red}{+7} & 1 \end{bmatrix}
	\longrightarrow A = LU
\end{align} 
\item And as it turns out it is \textit{much} easier to calculate the inverse of a triangular matrix than a dense matrix. We can rely on $A^{-1} = (LU)^{-1} = U^{-1}L^{-1}$, and then on the kind fact that we can find each of those inverses through back substitution of $U$ and forward-substitution of $L$.
\item In reality, it's worth noting that it's not \textit{really} $A=LU$, but rather $PA=LU$. We must apply a permutation matrix to $A$ in order to account for the fact that the pivots of $A$ may not be perfectly ordered. We may at some point be forced to do a row swap, and hence apply a permutation at the end of the computation.

\subsubsection*{CR Factorization}
\item This is not one of our ``critical factorizations''. However, it is useful for us. We learned a moment ago how to compute a basis for $C(A)$. We can use that to write $A = CR$, where $C \in \R^{m \times r}$ consists of \textit{any} basis for $C$. The truth of this should be intuitive by how we have constructed the principle of a columnspace. We can take for example
	$$A = \begin{bmatrix}
	  1 & 2 & 3 & 1\\
	1 & 2 & 5 & -3\\
	1 & 2 & 7 & -7
	\end{bmatrix} \leadsto U= \begin{bmatrix}
	  1 & 2 & 0 & 7\\
	 &  & 1 & 2\\
	 &  &  & 
	\end{bmatrix}$$
	where clearly $\mathbf{c}_{1}$ and $\mathbf{c}_{3}$ are the pivot rows such that $C(A) = \span \{\mathbf{c_{1}}, \mathbf{c}_{3}\}$. We can from this express a \textit{specific case} of CR-Factorization as
	$$A = \underbrace{ \begin{bmatrix}
	  1 & 3\\
	1 & 5\\
	1 & 7
\end{bmatrix}}_C \underbrace{ \begin{bmatrix}
	  1 & 2 & 0 & 7\\
	0 & 0 & 1 & 2
\end{bmatrix}}_R$$
where, here, $C$ is the \textit{specific} column space basis corresponding to the pivot columns of the original matrix, and $R$ is the reduced row eschelon form of $A$, ommiting the trivial last row. This does not generalize to all solutions for the factorization.

\subsection{Orthogonality}
\item Let us recall that for $x, y \in \R^{n}$, i.e. two ``n-component vectors", we can define the dot product (a.k.a. the inner product) as $x^{T}y = \begin{bmatrix}
		x_{1} & x_{2} & \cdots
	\end{bmatrix} \vthreeC{y_{1}}{y_{2}}{\vdots} = x_{1}y_{1}+ x_{2}y_{2} + \cdots$. From this we can define the $L^{2}$ \textbf{norm} or length of $x$ as $\|x\| = \sqrt{x^{T}x} = \sqrt{x_{1}^{2} + x_{2}^{2} + \cdots}$.
\item Also we can recall the key property that a matrix transpose moves the matrix to the other side of a dot product. For example $u \cdot (Av) = u^{T}Av = (A^{T}u)^{T}v = (A^{T}u)\cdot v$
\item In the two most common cases, orthogonal, and parallel, we will find nice properties. For $x \perp y$, $x^{T}y = 0$; for $x \parallel y$, $x^{T}y = \|x\| \|y\|$. 
\item In our original basis, we made no promises about what vectors defined the basis of the column space. This means that in order to solve $Ax=b$, we need to solve the whole system naively, which is computationally expensive. If we were to have a nicer basis, we could optimize some of that computation and inelegance. Imagine we have an \textbf{orthonormal basis} for a matrix. That is, the basis vectors of the matrix are orthogonal, and have magnitude 1. We will work on the details of how to compute one of these bases soon. For basis vectors $\mathbf{q}_{1}, \mathbf{q}_{2}$, we could define $\mathbf{b} = c_{1} \mathbf{q}_{1} + c_{2} \mathbf{q}_{2}$ and $\| \mathbf{b} \| = b^{T}b = (c_{1} \mathbf{q}_{1} + c_{2} \mathbf{q}_{2})^{T} (c_{1} \mathbf{q}_{1} + c_{2} \mathbf{q}_{2}) = c_{1}^{2} + c_{2}^{2}$, or, the Pythagorean Theorem.
\item In matrix form $Q = \begin{bmatrix}
		\mathbf{q}_{1} & \mathbf{q}_{2}
	\end{bmatrix}$, we have desirable properties such as $Q^{T}Q = I$, assuming that $Q$ is square, meaning that the transpose of $Q$ is it's inverse! If $Q$ is \textbf{not square}, we can say a lot less about it. It does not follow $Q^{T} = Q^{-1}$ and $Q^{T}Q \neq QQ^{T}$.
\item We can also define orthogonal subspaces $S_{1}$ and $S_{2}$ of $V$ if every vector in $S_{1}$ is orthogonal to every vector in $S_{2}$, or $x^{T}y = 0 \ \forall \ x \in S_{1}, y \in S_{2}$. Also, $\dim S_{1} + \dim S_{2} \leq \dim V$.
\item Finally, we can define orthogonal complements of subspaces $S^{\perp}$ for the subspace containing \textit{all} vectors in $V$ that are othogonal to \textit{every} vector in $S$. Therefore, $S^{\perp}$ is the largest subspace that is orthogonal to $S$. It is formally defined as $S^{\perp} = \{ w \in V : w^{T}x = 0 \text{ for any } x \in S\}$
	\begin{examplebox}[1.5]
	\noindent Denote a subspace $V = \Biggl\{\vthreeC{v_{1}}{v_{2}}{v_{3}} : 2v_{1} + 3v_{2} + 5v_{3}=0\Biggl\}$. Find $V^{\perp}$ (give a basis). Can you relate $V$ and $V^{\perp}$ to column and/or null spaces of some matrix. 
		\begin{solution}[1.5]\quad\\
			As we defined $V^{\perp} = \{w : w\cdot v = 0 \text{ for any } v \in V\}$. For $v = \begin{bmatrix}$
			  v_{1} & v_{2} & v_{3}
			\end{bmatrix}^T, $w = \begin{bmatrix}
			  2 & 3 & 5
			\end{bmatrix}^T$ such that $v \cdot w = 0$ as given. Therefore, $V^{\perp} = \operatorname{span} \Biggl\{ \vthreeC{2}{3}{5}\Biggl\}$. Alternatively $A \equiv \begin{bmatrix}
			  2 & 3 & 5
			\end{bmatrix}$ such that $Av = 0 \implies V = N(A)$ and $V^{\perp} = C(A^{T})$.
		    
		\end{solution}
		\begin{takeaways}[1.5]\quad
		    \begin{itemize}
				\item Remember the formal definition of the orthogonal subspace: it is just the subspace for which the dot product with \textit{anything} is 0. 
				\item This orthogonal subspace is highly related to the nullspace. Always be looking for the nullspace and columnspace connctions.
		    \end{itemize}
		\end{takeaways}
	\end{examplebox}
	\begin{examplebox}[1.6]\quad\\
		Suppose we have a subspace $\mathcal{S}$ with orthogonal (but not necessarily orthonormal) basis $\{ v_1, \cdots v_k\}$
		$$v = \sum_{i=1}^{k} \alpha_{i}v_{i}$$
	   by the definition of a basis, for some cosntants $\alpha_1, \ldots \alpha_k$. Determine every $\alpha_{j}$ in terms of $v_{i}, \ldots v_{k}$. Will this work if the basis is not orthogonal? If we put the vectors $v_{i}$ as columns of a marix $V = \begin{bmatrix}
	     v_{1} & \cdots & v_{k}
	   \end{bmatrix}$, what special form does $V^{T}V$ have?
	   \begin{solution}[1.6]\quad\\
	       In order to find $\alpha_{j}$ specifically, left multiply by $v_{j}^{T}$ such that 
		   $$v_{j}^{T}v = \sum_{i=1}^{k} v_{j}^{T}(\alpha_{i}v_{i}) = \sum_{i=1}^{k} \alpha_{i} v_{j}^{T}v^{i} = \alpha_{j} \|v_{j}\|$$
		   The genius of muliplying by $v_{j}^{T}$ is that for all terms $i \neq j$, $v_{j}^{T}v_{i} = 0$ such that we are able to get rid of that nasty summation. This leads easily to $\alpha_{j} = \dfrac{v_{j}^{T}v}{\|v_{j}\|^2}$. Then if we take the matrix $V$, we will end with just 
		   $$V^{T}V = \begin{bmatrix}
		     \|v_{1}\| &  &  & \\
		    & \|v_{2}\| &  & \\
		    &  & \ddots & \\
		    &  &  & \|v_{k}\|
		   \end{bmatrix}$$
		   since $V$ forms a linear multiple of the basis vectors.
	   \end{solution}
	   \begin{takeaways}[1.6]\quad
		   \begin{itemize}
			   \item The trick of left-multipling by $v_{j}^{T}$ is tricky as hell here yet. A good problem solving technique is to look for what you're trying to isolate and then the conditions that can help you simplify the opaque linear algebra expressions.
				\item For the basis vectors, it's important to realize this property of $v^{T}V$ that we will revisit on the section on symmetry later.
		   \end{itemize} 
	   \end{takeaways}
	\end{examplebox}
\end{itemize}
	\subsubsection*{Projections}

	\begin{wrapfigure}{l}{0.34\textwidth}
		\begin{tikzpicture}[scale=2.333]
	\clip (-0.6, -0.5) rectangle (2.0, 1.5);
	 \path (-0.6, -0.5) rectangle (2.0, 1.5); % Explicit bounding box
	\draw (-1.5, 0) -- (1.5, 0);    
	\draw (0, 1.5) -- (0, -1.5);
	\draw [thick, blue] (45:-1.8cm) -- node[very near end, anchor=west] {$S=C(A)$} (45:1.8cm);
	\draw [thick, dotted, blue] (-45:-1.0cm) -- node[near start, anchor=south, yshift=2mm] {$S^{\perp}$} (-45:0.7cm);
	\draw [->] (0, 0) -- node[very near end, anchor=east] {$\vec{b}$} (75: 1.3cm);
	\coordinate (t) at (intersection of 75:1.3cm -- ++ 15:1.3cm and 0,0--45:2cm);
	\draw [->] (75:1.3cm) -- node[midway, anchor=south] {$\vec{e}$} (t);
	\draw [thick, red, -> ] (0,0)-- node[midway, anchor=west] {$\vec{p} = A \hat{x}$} (t);	
\end{tikzpicture}
\end{wrapfigure}

\parshape If we imagine some $n$-dimensional subspace $S \subseteq \R^{n}$, we can define a vector $\mathbf{b}$ that exists outside the subspace. We can also define a vector $\mathbf{p}$, the orthogonal projection of $\mathbf{b}$ onto $S$. Here we could say 
$$\mathbf{b} = \underbrace{\mathbf{p}}_{\in S} + \underbrace{\mathbf{e}}_{\in S^{\perp}} = P \mathbf{b} + (I -P) \mathbf{b}$$
For our ``projection matrix'' $P$. Now, let's imagine we define $S= C(A)$ such that $S^{\perp}= N(A^{T})$. Then, $\mathbf{p} = A \hat{x} \in C(A)$, and $\hat{x}$ is the closeset solution to $\mathbf{b}$. Therefore, $\mathbf{e} = \mathbf{b} - A\hat{x} \in N(A^{T}) \to A^{T} (b-A\hat{x}) \to A^{T}b = A^{T}A\hat{x}$. From this, we can define three \textbf{normal equations}. 
\begin{equation*}
\boxed{
\begin{align*}
			\hat{x} &= (A^{T}A)^{-1}A^{T}b \\
			\mathbf{p} &= A \hat{x} = P \mathbf{b} \\
			P &= A(A^{T}A)^{-1}A^{T}
	\end{align*}}
\end{equation*}
\begin{itemize}
	\item We can also define a handful of neat properties about the projection matrix $P$.
		\begin{enumerate}
			\item $P^{2} = P \hspace{2cm} \text{Proof.  } A(A^{T}A)^{-1}(A^{T}A)(A^{T}A)^{-1}A^{T} = P$
			\item $P^{T} = P \hspace{2cm} \text{Proof.  } \qty((A^{T}A)^{-1}A^{T}b)^{T} = (A^{T})^{T} \qty((A^{T}A)^{-1})^{T} A^{T} = P$
			\item $C(P)= C(A)$
			\item $N(P)= C(A)^{\perp} = N(A^{T})$
			\item $I-P$ is the projection matrix onto $N(A^{T})$

		\end{enumerate}
	\item Our projection vector $\mathbf{p}$ is going to minimize the norm $\|b-Ax\|$ as should be clear in the geometry above. Notably, it won't necessarily come upwith an \textit{exact solution}. Rather it is able to find the closest solution when there is no absolute solution. This is the beginning of our forees into optimization. As an example, let's examine the simplest optimization problem: least squares.
\end{itemize}

\begin{examplebox}
		\begin{wrapfigure}{l}{0.4\textwidth}
		\includegraphics[width=\linewidth]{figs/linreg.png}
    \end{wrapfigure}
	\nident Given a time series of temperature data, we want to calculate the linear regression with our matrix methods. We are going to construct another Vandermonde matrix: this time with a highly overdetermined system. 
	$$\begin{pmatrix}
	  1 & y_1 - y_0\\
	1 & y_2 - y_0\\
	\vdots & \vdots\\
	1 & y_m - y_0
	\end{pmatrix}\begin{pmatrix}
	  x_1 \\
	x_2
	\end{pmatrix}
	= \begin{pmatrix}
	  \Delta T_1\\
	\Delta T_2\\
	\vdots\\
	\Delta T_m
	\end{pmatrix}$$

	To find our solution, we must simply minimize the orthogonal projection with the normal equations
	$$\displaystyle \argmin_{x \in \R^{2}} \|b-Ax\| = (A^{T}A)^{-1}A^{T}b$$
\end{examplebox}
	\vspace{0.5cm}
	\begin{itemize}
\end{itemize}

\end{document}

