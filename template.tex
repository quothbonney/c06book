\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amssymb, amsthm, amsmath}
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
  
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

\makeatother

\usepackage{wrapfig}
\usepackage{quick}


\linespread{1.03} % give a little extra room
\setlength{\parindent}{0.2in} % reduce paragraph indent a bit
\setcounter{secnumdepth}{2} % no numbered subsubsections
\setcounter{tocdepth}{2} % no subsubsections in ToC
\lhead{\sffamily The 18.C06 Master Document}
\begin{document}

% make title page
\thispagestyle{empty}
\bigskip 
\vspace{0.1cm}

\begin{center}
		{\fontsize{20}{20} \selectfont \bf \sffamily The 18.C06 Master Document}
	\vskip 12pt
		{\fontsize{18}{18} \selectfont \rmfamily Jack D.V. Carson}
	\vskip 6pt
		{\fontsize{14}{14} \selectfont \ttfamily jdcarson@mit.edu}
	\vskip 24pt
\end{center}


% make table of contents
\microtoc
\section{Linear Algebra}
This document assumes you already have a reasonable familiarity with the most basic ideas of linear algebra (i.e. What is a matrix? What is matrix multiplication? Why would one be compelled to multiply a matrix?). From hence, we shall begin with the beginning.
\subsection{The Basics}
\begin{itemize}
	\item    In linear algebra, we most often want to express some situation in terms of a formula $Ax=b$ for a matrix $A$, and vectors $x, b$. Then, once we have expressed them in this form, we wish to solve for the values of $x$ that give $b$. The simplest way to do this is with \textbf{Gaussian Elimination} wherein rows are added to one another or multiplied by scalar constants to give a \textbf{diagonal matrix}. This algorithm always goes columnwise. I.e. start from the first column and try and clear everything such that everything below the pivot is 0. This may not always be the fastest approach but it is consistent. From this diagonal matrix, it is simple to \textbf{backsubstitute} to get the complete values of $x$.
		$$\begin{bmatrix}[ccc|c]
  1 & 3 & 1 & 9\\
1 & 1 & -1 & 1\\
3 & 11 & 6 & 35
\end{bmatrix} \xrightarrow{\substack{r_{2}-r_{1} \\ r_{3}-3r_{1}}} \begin{bmatrix}[ccc|c]
  1 & 3 & 1 & 9\\
0 & -2 & -2 & -8\\
0 & 2 & 3 & 8
\end{bmatrix} \xrightarrow{r_{3}+r_{2}}\begin{bmatrix}[ccc|c]
  1 & 3 & 1 & 9\\
0 & -2 & -2 & -8\\
0 & 0 & 1 & 0
\end{bmatrix}$$
Here we have gone from the augmented matrix $[ A | b]$ to a much nicer $[U | b]$. By recalling that matricies are just an abstraction of linear functions, we can look at the last row and say that $x_{3}=0$. We can \textit{propogate} this upwards to say therefore that $-2x_{2}+0=-8 \to x_{2}=4$
\item If we get a 0 in a pivot position, we say that a matrix is \textbf{singular}. In this case $Ax=b$ may not have a solution, or it may have infinite solutions. But it certainly does not have a single solution. All linear equations of this type either have 0, 1, or $\infty$ solutions.
\item It wouldn't hurt us to brush up on matrix multiplication either. It turns out there are many ways to think about this operation.
	\begin{enumerate}
		\item \textbf{Entry-wise:} For each $1 \leq i \leq m$ and $1 \leq j \ldq p$ we have
			$$C_{ij}= \sum_{k=1}^{n} A_{ik}B_{kj}$$
		\item \textbf{Inner product:} $C_{ij}$ is the dot product (also known as ``inner product'') of the $i$th row in $A$ and the $j$th column in $B$. For instance,
			$$C = \begin{bmatrix}
			  \horzbar & x_{1} & \horzbar\\
			  \horzbar & x_{1} & \horzbar\\
					   & \vdots & \\
			  \horzbar & x_{1} & \horzbar
			\end{bmatrix} \begin{bmatrix}
			  \vertbar & \vertbar & &\vertbar \\
			  b_{1} & b_{2} & \cdots & b_{p} \\
			  \vertbar & \vertbar & &\vertbar 
			\end{bmatrix} = \begin{bmatrix}
			  x_{1} \cdot b_{1} & x_{1} \cdot b_{2} & \cdots  & x_{1} \cdot b_{p}\\
			x_{2} \cdot b_{1} & x_{2}\cdot b_{2} & \cdots & b_{2} \cdot b_{p}\\
			\vdots & \vdots & \ddots & \vdots\\
			x_{m}\cdot b_{1} & x_{m} \cdot b_{2} & \cdots & x_{m} \cdot b_{p}
			\end{bmatrix}$$
		\item \textbf{Column-wise: }the $j$th column of a matrix $C$ is the matrix-vector product of $A$ and the $j$th column of $B$. For instance,
			$$C = A \begin{bmatrix}
			  \vertbar & \vertbar &  & \vertbar\\
			b_{1} & b_{2} & \cdots & b_{p}\\
			  \vertbar & \vertbar &  & \vertbar
			\end{bmatrix} = \begin{bmatrix}   
			  \vertbar & \vertbar &  & \vertbar\\
			Ab_{1} & Ab_{2} & \cdots & Ab_{p}\\
			  \vertbar & \vertbar &  & \vertbar
			\end{bmatrix}$$
		\item \textbf{Outer product:} $C$ is the sum of the product of $i$th column of $A$ and the $i$th row of $B$ such as
			$$C=\begin{bmatrix}
			  \vertbar & \vertbar &  & \vertbar\\
			a_{1} & a_{2} & \cdots & a_{n}\\
			\vertbar & \vertbar &  & \vertbar
			\end{bmatrix}
			\begin{bmatrix}
			  \horzbar & y_{1} & \horzbar\\
			\horzbar & y_{2} & \horzbar\\
			 & \vdots & \\
			\horzbar & y_{n} & \horzbar
			\end{bmatrix} = 
			\sum_{i=1}^{n} \begin{bmatrix}
			  \vertbar \\
			a_{i}\\
			\vertbar
			\end{bmatrix} \begin{bmatrix}
			  \horzbar & y_{i} & \horzbar
			\end{bmatrix}$$
	\end{enumerate}
	\begin{ideabox}[Properties of Matrix Multiplication]\quad
		\begin{enumerate}
			\item \textbf{Associative: } $A(BC) = (AB)C$
			\item \textbf{Distributive: } $A(B+C) = AB + AC \iff (A+B)C = AC+BC$
			\item \textbf{Non-commutative: } $AB \neq BA$
			\item \textbf{Identity: } $IA = AI = A$
		\end{enumerate} 
	\end{ideabox}
\item Although it can often be lost in the abstraction of mathematics, matricies really, truly are \textbf{linear operators}. They transform spaces and vectors to other spaces and other vectors. As an example consider 2023 Recitation 1 Problem 1
	\begin{examplebox}[1.1]
		\begin{enumerate}
	\item Find a $2 \times 2$ matrix such that when you multiply a 2-D vector by it, the result is a reflection of the vector across the origin
	\item Find a $3 \times 3$ matrix such that when you multiply a 3-D vector by it, it swaps the second and third coordinates.
	\item If you have a $4 \times 4$ matrix $A$, find a 4-D vector $x$ such that $Ax$ is the second column of $A$.
		    
		\end{enumerate}
		\begin{solution}[1.1] \quad\vspace{-0.2cm}
		\begin{enumerate}
		    \item To reflect across the origin, $x \mapsto -x$, $y \mapsto -y$. Therefore, $A = \begin{bmatrix}
	  -1 & 0\\
	0 & -1
	\end{bmatrix}$.

\item Here $\begin{bmatrix}
  x\\
y\\
z
\end{bmatrix} \mapsto \begin{bmatrix}
  x\\
z\\
y
\end{bmatrix} = x \begin{bmatrix}
  1\\
0\\
0
\end{bmatrix} + y\begin{bmatrix}
  0\\
0\\
1
\end{bmatrix} + z \begin{bmatrix}
  0\\
1\\
0
\end{bmatrix} = \underbrace{\begin{bmatrix}
  1 & 0 & 0\\
0 & 0 & 1\\
0 & 1 & 0
\end{bmatrix}}_A \begin{bmatrix}
  x\\
y\\
z
\end{bmatrix}$

Here $A$ is a \textbf{permutation matrix}, as it permutes one or more of the variables. Permutation matricies have a variety of desirable properties such as $A^{-1}= A^{T}$
\item Since we are only interested in the second column, we want the products $\mathbf{c_{i}}x_{i}=0$ for $i=1,3,4$ to be 0. The only way to guaruntee this is to specify $x_{1,3,4}=0$ and $x_{2}=1$. Therefore $\mathbf{x}=\begin{bmatrix}
  0 & 1 & 0 & 0
\end{bmatrix}^{T}$
		\end{enumerate}
	\end{solution}
	\begin{takeaways}[1.1]\quad\vspace{-0.5cm}
	    \begin{enumerate}
			\item All of the matrix formulations here are useful to recognize, particularly the intuition of part (3). This will be seen many times.
			\item One useful way to interpret the result in part (2) is the fact that left-multiplied matricies operate on the rows of that which they multiply. Right-multiplied matricies operate on columns. Because $\mathbf{x}$ has only one column, right-multiplication is nonsensical. 

	    \end{enumerate}
	\end{takeaways}
	\end{examplebox}
	\begin{examplebox}[1.2]\quad
	    \begin{enumerate}
			\item Find a $3 \times 3$ matrix $P$ such that in $B=PA$ is the result of subtracting the second row from the third row of $A$ and then swapping the first and second rows.
			\item Find a $4 \times 4$ matrix that right multiplies $A$ such that result $C=AQ$ is $A$ after dividing the first column by two, and then adding the first column to the second and third columns
			\item Does the order of performing the operations in (1) and (2) matter?
	    \end{enumerate}
		\begin{solution}[1.2]\quad\vspace{-0.5cm}
		    \begin{enumerate}
				\item $P=\underbrace{\begin{bmatrix}
				  1 &  & \\
				 & 1 & \\
				 & -1 & 1
		  \end{bmatrix}}_{r_{3}=r_{3}-r_{2}}\underbrace{ \begin{bmatrix}
		     & 1 & \\
		  1 &  & \\
		   &  & 1
  \end{bmatrix}}_{\text{swap } \bm{c}_{1}, \bm{c}_{2}} = \begin{bmatrix}
   0  & 1 &0 \\
  1 & 0 &0 \\
  0 & -1 & 1
  \end{bmatrix}$
\item $Q = \begin{bmatrix}
  1/2 &  &  & \\
 & 1 &  & \\
 &  & 1 & \\
 &  &  & 1
\end{bmatrix} \begin{bmatrix}
  1 & 1 & 1 & \\
 & 1 &  & \\
 &  & 1 & \\
 &  &  & 1
\end{bmatrix} = \begin{bmatrix}
  1/2 & 1/2 & 1/2 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{bmatrix}$. This multiplication should be obvious by the column-wise definition above.
\item Although it might look like we are dealing with the commutative propety here, we are really dealing with the associative property. In (1). $B=(P_{1}P_{2})A = P_{1}(P_{2} A)$. So the order of operations does not matter. In (2) it is the same $C=A(Q_{1}Q_{2}) = (AQ_{1})Q_{2}$
		    \end{enumerate}
		\end{solution}
		\begin{takeaways}[1.2]\quad\vspace{-0.4cm}
		    \begin{itemize}
				\item Remember that left multiplication always affects rows only, and that right multiplication affects columns only. Complex operations can be formed by chaining linear operators together.
				\item Matrix multiplication is associative! When doing these chained operations, the order does not matter.
				\item Remember the forms that these kinds of matricies take. They are not always obvious.
		    \end{itemize}
		\end{takeaways}
	\end{examplebox}
\item Before we close up, there are some other noteworthy operations we can perform with matricies that will follow us around. The \textbf{transpose} of a matrix $A^T$ ``flips'' a matrix $A$ such that 
	$$A = \begin{bmatrix}
	  1 & 2 & 3\\
	4 & 5 & 6
	\end{bmatrix}\quad\quad A^{T} = \begin{bmatrix}
	  1 & 4\\
	2 & 5\\
	3 & 6
	\end{bmatrix}$$
	and with it we can define certain identities such as $\boxed{(A^T)_{i,j} = A_{j, i}}$ and $\boxed{(AB)^{T} = B^{T}A^{T}}$.Finally, we can define the dot product between two vectors $\mathbf{a} \cdot \mathbf{b} = \mathbf{a}^{T}\mathbf{b}$. All three of these facts will show up constantly. 
\item Last but not least, the \textbf{inverse} $A^{-1}$ of a matrix is the matrix such that $AA^{-1}=A^{-1}A=I$. More interestingly we can say that in a situation $Ax=b$, $x = A^{-1}b$. This matrix inverse does not always exists, and is exceedingly impractical to calculate for large matricies, an issue we will deal with thoroughly in optimization. A matrix is said to be \textbf{invertible} if and only if it is square and full column rank. That is, every column has a pivot. The inverse, if it exists, is always unique. Like the transpose it is subject to the identity $\boxed{(AB)^{-1} = B^{-1}A^{-1}}$
\item We can define more facts that will help us with these two operations. For instance $(A^T)^T$ and $(A+B)^{T} = A^{T}+B^{T}$, which follow directly from the definition of the transpose. More useful is $\boxed{(A^{-1})^{T} = (A^{T})^{-1}}$. All of these identities will help us greatly.
\end{itemize}
\subsection{Vector Spaces}
\begin{ideabox}[Definition]
    A \textbf{vector space} $V$ is a set of elements (e.g., vectors in $\R^{n}$, polynomials, diagonal $2 \times 2$ matricies) defined over a ``field'' $F$ of scalars that are closed under
	\begin{enumerate}
		\item \textbf{Vector Addition: } For any vectors $\bm{u}$ and $\bm{v}$ in $V$, $\bm{u} \pm \bm{v}$ also belongs the vector space $V$.
		\item \textbf{Scalar Multiplication: }For any vector $\bm{v}$ in $V$ and scalar $c$ in $F$, $c\bm{v}$ is an element of the vector space
	\end{enumerate}
\end{ideabox}
\begin{itemize}
	\item If $W \subseteq V$ is also a vector space with respect to the operations in $V$, $W$ is a \textbf{subspace} of $V$. Specifically for any $v, w \in W$, $v+w \in W$ and $cv \in W$. For instance of we define a vector space $\R^2$. $W$ could be $\R^2$ or $W= \{0\}$. A more enlightening sample vector space is a line that passes through the origin (note that a line that does not pass through the origin would not be a vector space since $\alpha v \not \in V$ for $\alpha =0$). Every point on this line is closed under addition and scalar multiplication.
	\item There are an infinite number of obscure facts to say about spaces and subspaces. More interestingly, is how to show something is a space or a substapce. We can be convinced that for subspaces $S_{1}$ and $S_{2}$ $S = S_{1} \cap S_{2}$ is also a subspace by showing that $S$ is closed under addition and scalar multiplication.
		\\\\
		\begin{proof} Assuming that this is true, any $v, w \in S$ will also be $v, w \in S_{1}$ and $v,w \in S_{2}$ by the definition of an intersection. Then $v+w$ will be in	$S_{1}$ and $S_{2}$ by the deifnition of subspace. Thus $v+w \in S_{1} \cap S_{2} =S$ which shows that it is closed under addition. Then we can define $v \in S_{1} \cap S_{2}$ and $\alpha \in \R$. Since $S_{1}$ is a subspace, $\alpha v \in S_{1}$ and $S_{2}$ such that $\alpha v \in S_{1} \cap S_{2}=S$, showing that it is closed under scalar multiplication. 
		\end{proof}
	\end{itemize}
		\begin{ideabox}[Common vector spaces]\quad
			\begin{itemize}
			    
		    
\item The \textbf{column space} of an $m \times n$ matrix $A$, denoted $C(A)$ is the set of linear combinations of columns of $A$, also known as the \textbf{span} of $A$. Although the idea may seem a bit abstract, soon we will be very interested in what columnspace a vector is in. We can define useful facts such as
	\begin{enumerate}
		\item Formally, $C(A) = \{Ax \mid x \in \R^{n}\}$ 
		\item $Ax=b$ has a solution if and only if $b \in C(A)$
		\item If $m=n$, then $A$ is invertible if and only if $C(A) = \R ^{n}$
	\end{enumerate}
\item The \textbf{null space} of $A$, denoted $N(A)$ is the set of vectors $x$ such that $Ax = \mathbf{0}$. We can define similar facts
	\begin{enumerate}
		\item $N(A) = \{ x \in \R^{n} \mid Ax = \bm{0} \}$
		\item If $B$ is square and invetible, $N(A)=N(BA)$
		\item If $A \in \R^{n \times n}$, then $C(A) = \R^n$ is equivalent to $N(A) = \{\bm{0}\}$
	\end{enumerate}
\end{itemize}
		\end{ideabox}
		\begin{itemize}
			\subsubsection*{Computing basis of a null space}
		\item Before we start with computation, it helps to briefly examine our matrix. If we are looking at an invertible matrix, then its null space will clearly be $\{ \bm{0}\}$, since there is no non-trivial $x$ that will give $\bm{0}$. Even if $A$ is not invertible, if every column has a pivot, then there are no variables that can move freely. This can happen easily for an ``overdetermined'' system where a matrix has more rows than columns. However, the most interesting case in which $A$ is ``underdetermined'' such that it has many more columns than rows, and thus many free variables.
		\item Our goal for underdetermined systems will be to transform some matrix $A$
			$$A \leadsto U = \begin{bmatrix}
			  U_r & F \\
			  m-r \text{ rows of }0's & \cdots
			\end{bmatrix}$$
		\item Now how do we implement this scary looking transformation? In reality it is quite simple. First transform $A$ into an upper-triangular matrix $U$ with gaussian elimination.

		\begin{align}
       A = \begin{bmatrix}
         1 & 2 & 3 & 1\\
       1 & 4 & 5 & -3\\
       1 & 6 & 7 & -7
       \end{bmatrix}
	   \leadsto
	   \begin{bmatrix}
		   \boxed{1} & 2 & 3 & 1\\
		 & \boxed{2} & 2 & -4\\
	    &  &  & 
	   \end{bmatrix}
   \end{align} 
   Let pivot columns $U_r =\begin{bmatrix}
     1 & 2\\
    & 2
   \end{bmatrix}$ and free columns $F=\begin{bmatrix}
     3 & 1\\
   2 & -4
   \end{bmatrix}$
   such that
       $U=\begin{bmatrix}
         U_r & F\\
		 \mathbf{0} & \mathbf{0}
       \end{bmatrix}$.
   \item Then we can start with the actual linear algebra. To find $N(A)$ we want $Ax = \mathbf{0} \iff Ux = \mathbf{0}$. In order to compute this we define two vectors $\mathbf{p}$ for the coefficients of $x$ by which the pivot columns of $U$ are multiplied, and a vector $f$ for the values of $x$ that multiply the free columns. With this we can say,
   \begin{gather}
	   U=\begin{pmatrix}
         U_r & F\\
		 \mathbf{0} & \mathbf{0}
       \end{pmatrix}
	   \begin{pmatrix}
		   \mathbf{p}\\
		   \mathbf{f}
	   \end{pmatrix}
		= \begin{pmatrix}
		   U_r \mathbf{p} + F\mathbf{f}\\
		   \mathbf{0}
	   \end{pmatrix}
	   = \mathbf{0}  \\
   U_r \mathbf{p} + F\mathbf{f} = \mathbf{0} \\
   \boxed{U_r \mathbf{p} =- F\mathbf{f}}
   \end{gather}
   $U_r$ is guarunteed to be invertible, so we can say
	   $\mathbf{p} = U_{r}^{-1}(-F \mathbf{f})$
   is guarunteed to be uniquely determined for any choice of $\mathbf{f}$. Then $\begin{pmatrix}
	   \mathbf{p}\\
	   \mathbf{f}
   \end{pmatrix}$ is a basis vector of our nullspace. To show this, let's expand the boxed formula above for our example.
   \begin{align}
       \begin{bmatrix}
         1 & 2\\
        & 2
       \end{bmatrix}
	   \begin{bmatrix}
	     p_{1}\\
	   p_{2}
	   \end{bmatrix}
	   =-\begin{bmatrix}
	     3 & 1\\
	   2 & -4
	   \end{bmatrix}
	   \begin{bmatrix}
	     f_{1}\\
	   f_{2}
	   \end{bmatrix}
   \end{align}
\item For an $n$-dimensional null space, we can just make up whatever $n$ linearly independent $\mathbf{f}$ vectors we might fancy. For simplicity, say $\mathbf{f} = \begin{bmatrix}
  1\\
0
\end{bmatrix}$ such that
   \begin{align}
       \begin{bmatrix}
         1 & 2\\
        & 2
       \end{bmatrix}
	   \begin{bmatrix}
	     p_{1}\\
	   p_{2}
	   \end{bmatrix}
	   =\underbrace{\begin{bmatrix}
	     -3 \\ -2\\
 \end{bmatrix}}_{-F \mathbf{f}} \longrightarrow \mathbf{p}= \vtwoC{-1}{-1} \text{ and }x = \begin{bmatrix}
   -1 & -1 & 1 & 0
\end{bmatrix}^{T}
   \end{align}
   Then we can do this again for $\mathbf{f} = \begin{bmatrix}
     0 & 1
   \end{bmatrix}^{T}$ to complete the basis for $N(A)$.
   
\item In the case that the pivot columns are not adjacent, you \textit{can} interlace the $p$ and $f$ elements systematically.

\subsubsection*{Computing the basis of a columnspace}
\item Let's revisit our matrix $A$ from the previous example.
\begin{align}
    A = \begin{bmatrix}
      1 & 2 & 3 & 1\\
    1 & 4 & 5 & -3\\
    1 & 6 & 7 & -7
    \end{bmatrix}
	\leadsto
	U= \begin{bmatrix}
	  1 & 2 & 3 & 1\\
	 & 2 & 2 & -4\\
	 &  &  & 0
	\end{bmatrix}
\end{align}
In $U$ we can easily identify that $c_{1}$ and $c_{2}$ contain our pivots. Therefore, our basis for $C(A)$ is simply columns $c_{1}$ and $c_{2}$ of $A$ itself. Don't forget, though, that $C(A) \neq C(U)$. Rather,
\begin{align}
    C(A) = \text{span}\Biggl\{\begin{bmatrix}
	  1\\
	1\\
	1
	\end{bmatrix}, \begin{bmatrix}
	  2\\
	4\\
	6	
	\end{bmatrix} \Biggl\}
\end{align}

\begin{examplebox}[1.3]\quad
	\begin{enumerate}
		\item If the zero vector is in $C(A)$, then the columns of $A$ are linearly dependent.
		\item The columns of a matrix are a basis for the column space
		\item Define the row space of $A$ as the span of the row vectors. If $A$ is square, then the row space equals the column space.
		\item The row space of $A$ is equal to the column space of $A^{T}$.
		\item If the row space of $A$ equals the column space, then $A^{T} = A$.
		\item A $4 \times 4$ permutation matrix has $C(P) = \R^{4}$.
		\item For $ v \in N(A) $, if $x$ is a solution to $Ax=b$, so is $x + v$
	\end{enumerate}
	\begin{solution}[1.3]\quad\vspace{-0.4cm}
		\begin{enumerate}
			\item \textbf{False.} $A=I$ is a counterexample. The zero vector is in the column span of every matrix!
			\item \textbf{False.} This is true only if the columns are linearly independent. It is clearly not true for underdetermined sysystems where there are more columns than rows.
			\item \textbf{False.} As a counterexample $A = \mtwtw{1}{1}{0}{0}$ where $C(A) = \vtwoC{1}{0}$ and $R(A) = \vtwoC{1}{1}$
			\item \textbf{True.} This is simply the definition of the row space, and the set of rows of $A$ is identical to the set of columns of $A^{T}$.
			\item \textbf{False:} Consider $A = \mtwtw{1}{2}{3}{4}$. Both $C(A) = \R^{2}$ and $R(A) = \R^{2}$. However, $A \neq A^{T}$.
			\item \textbf{True:} A permutation space simply permutes the order of the variables. If it were to not be in $\R^{4}$, it would somehow lose one of the dimensions.
			\item \textbf{True:} If $v \in N(A)$, $Av = \mathbf{0}$. Therefore $A(x+v) = Ax+av =b$ which is defined in $C(A)$ \end{enumerate}
	    
	\end{solution}
	\begin{takeaways}[1.3]\quad\vspace{-0.3cm}
		\begin{enumerate}
		    \item We will see the row space $C(A^{T})$ in more detail soon. This problem examines simple properties that it has. It is essential to remember that $C(A) \neq C(A^{T})$, even if they have the same dimension.
			\item Always look for simple matricies of size $2 \times 2$ or smaller tos how counterexamples. Most false properties will collapse even by this size.
		\end{enumerate}
	\end{takeaways}
    
\end{examplebox}

\begin{examplebox}[1.4]\quad
$AB = 0$ (the zero matrix) for matricies $A$ and $B$. If the null space of $\_\_\_$ is $\{=, \subseteq, \supseteq \}$ the column space of $\_\_\_$?
\begin{solution}[1.4]\quad\vspace{-0.3cm}
	Recalling our ``Column-Wise`` definition for matrix multiplication
			$$AB = \begin{bmatrix}   
			  \vertbar & \vertbar &  & \vertbar\\
			Ab_{1} & Ab_{2} & \cdots & Ab_{p}\\
			  \vertbar & \vertbar &  & \vertbar
			\end{bmatrix}$$
			$AB=0$ only if every column of $B$ is in the null space of $A$. Therefore the $N(A)$ must contain any possible linear combination of $B$ and possibly more. Therefore $N(B) \supseteq C(A)$
\end{solution} 
\end{examplebox}

\subsection{Factorization}
\item Before we get to factorization, we should define some simple concepts that have already appeared. For instance, the \textbf{rank} of a matrix is simply the number of pivots. Simply, $\operatorname{rank} A = \dim C(A) = \dim C(A^{T})$, which should be intuitive. We can also define the \textbf{rank-nullity theorem} as $\dim N(A) = n-r$ for $r$ pivots and $n$ columns. Although we do not yet have the tools to introduce them all now, in this document we will examine \textbf{four critical factorizations}
	\begin{align}
		A &= LU \\
		A &= QR \\
		S &= Q \Lambda Q^{T} \\
		A &= U \Sigma V^{T}
	\end{align}
	\subsubsection*{LU Factorization}
\item It turns out that matricies can be described in many ways. Some of these ways can help you solve difficult problems. The first of these is the $\bm{LU}$\textbf{-factorization}, which relates $A$ to its upper-triangular form $U$. In the process, it tells us about the inverse of a matrix. 
\item The crux of the factorization is just Gaussian elimiation of $A \leadsto U$, but storing the coefficients of each row operation in a left-multipliying matrix $L$ 
\begin{align}
	A &= \begin{bmatrix} \color{blue}{1} & 2 & 0 \\ 2 & 5 & 1 \\ -3 & 1 & -1 \end{bmatrix} \stackrel{r_2 - \color{red}{2}r_1}{\stackrel{r_3 + {\color{red}{3}}r_1}{\longrightarrow}}
    \begin{bmatrix} \color{blue}{1} & 2 & 0 \\ 0 & \color{blue}{1} & 1 \\  0 & 7 & -1 \end{bmatrix} \stackrel{r_3 - \color{red}{7}r_2}{\longrightarrow}
    \begin{bmatrix} \color{blue}{1} & 2 & 0 \\ 0 & \color{blue}{1} & 1 \\  0 & 0 & \color{blue}{-8} \end{bmatrix} = U  \\
	\implies L &= \begin{bmatrix} 1 & & \\ \color{red}{+2} & 1 & \\ \color{red}{-3} & \color{red}{+7} & 1 \end{bmatrix}
	\longrightarrow A = LU
\end{align} 
\item And as it turns out it is \textit{much} easier to calculate the inverse of a triangular matrix than a dense matrix. We can rely on $A^{-1} = (LU)^{-1} = U^{-1}L^{-1}$, and then on the kind fact that we can find each of those inverses through back substitution of $U$ and forward-substitution of $L$.
\item In reality, it's worth noting that it's not \textit{really} $A=LU$, but rather $PA=LU$. We must apply a permutation matrix to $A$ in order to account for the fact that the pivots of $A$ may not be perfectly ordered. We may at some point be forced to do a row swap, and hence apply a permutation at the end of the computation.
$
\subsubsection*{CR Factorization}
\item We learned a moment ago how to compute a basis for $C(A)$. We can use that to write $A = CR$, where $C \in \R^{m \times r}$ consists of \textit{any} basis for $C$. Then, the columns of $R$ are a basis for $C(A^{T})$. This has to be computed case-by-case, and it's computation is not of particular interest to us right now. 
\item 

\end{document}

